"""
BPE æ˜¯å­—èŠ‚å¯¹ç¼–ç å™¨çš„ç¼©å†™ã€‚å®ƒå°†ä»»æ„çš„ UTF-8 å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•´æ•°åºåˆ—ï¼Œå…¶ä¸­æ¯ä¸ªæ•´æ•°ä»£è¡¨ä¸€å°å—ç»å¸¸å‡ºç°çš„å­—ç¬¦ã€‚

æ­¤å®ç°åŸºäº OpenAI çš„ GPT-2 encoder.pyï¼šhttps://github.com/openai/gpt-2/blob/master/src/encoder.py

ä½†ç•¥æœ‰ä¿®æ”¹ï¼Œå› ä¸ºåŸå§‹å®ç°æœ‰ç‚¹ä»¤äººå›°æƒ‘ã€‚æˆ‘è¿˜å°è¯•æ·»åŠ å°½å¯èƒ½å¤šçš„æ³¨é‡Šï¼Œä»¥è§£é‡Šæˆ‘è‡ªå·±å¯¹ä»£ç çš„ç†è§£ã€‚
"""

import os
import json
import regex as re
import requests

import torch

# -----------------------------------------------------------------------------

def bytes_to_unicode():
    """
    OpenAI å°†æ¯ä¸ªå¯èƒ½çš„å­—èŠ‚ï¼ˆå®é™…ä¸Šæ˜¯æ•´æ•° 0..255ï¼‰æ˜ å°„åˆ°ä¸€ä¸ªç›´è§‚è¡¨ç¤ºå®ƒçš„ Unicode å­—ç¬¦ã€‚ä¸€äº›å­—èŠ‚ä¿ç•™äº†å®ƒä»¬çš„å¤–è§‚ï¼Œ
    å› ä¸ºå®ƒä»¬ä¸ä¼šé€ æˆä»»ä½•éº»çƒ¦ã€‚è¿™äº›å­—èŠ‚å®šä¹‰åœ¨åˆ—è¡¨ bs ä¸­ã€‚ä¾‹å¦‚ï¼šchr(33) è¿”å› "!"ï¼Œæ‰€ä»¥åœ¨è¿”å›çš„å­—å…¸ä¸­ï¼Œæˆ‘ä»¬ç®€å•åœ°å¾—åˆ° d[33] -> "!"ã€‚
    ç„¶è€Œï¼Œä¾‹å¦‚ï¼Œchr(0) æ˜¯ '\x00'ï¼Œè¿™çœ‹èµ·æ¥å¾ˆéš¾çœ‹ã€‚æ‰€ä»¥ OpenAI å°†è¿™äº›å­—èŠ‚æ˜ å°„åˆ°ä¸€ä¸ªæ–°çš„å­—ç¬¦èŒƒå›´å†…ï¼Œåœ¨è¿™ä¸ªèŒƒå›´å†…ï¼Œchr() 
    è¿”å›ä¸€ä¸ªç¾è§‚çš„å­—ç¬¦ã€‚æ‰€ä»¥åœ¨æœ€ç»ˆçš„å­—å…¸ä¸­ï¼Œæˆ‘ä»¬å¾—åˆ° d[0] -> 'Ä€'ï¼Œå®ƒå®é™…ä¸Šæ˜¯ chr(0 + 2**8)ã€‚
    ç‰¹åˆ«åœ°ï¼Œç©ºæ ¼å­—ç¬¦æ˜¯ 32ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ ord(' ') çœ‹åˆ°è¿™ä¸€ç‚¹ã€‚è¿™ä¸ªå‡½æ•°ä¼šå°†ç©ºæ ¼ (32) ç§»åŠ¨ 256 ä½åˆ° 288ï¼Œæ‰€ä»¥ d[32] -> 'Ä 'ã€‚
    æ‰€ä»¥è¿™åªæ˜¯ä¸€ä¸ªç®€å•çš„å­—èŠ‚ 0..255 åˆ° Unicode å­—ç¬¦çš„ä¸€å¯¹ä¸€æ˜ å°„ï¼Œè¿™äº›å­—ç¬¦â€œçœ‹èµ·æ¥å¾ˆæ¼‚äº®â€ï¼Œè¦ä¹ˆæ˜¯åŸå§‹å½¢å¼ï¼Œ
    è¦ä¹ˆæ˜¯ä¸€ä¸ªæœ‰è¶£çš„ç§»ä½å­—ç¬¦ï¼Œæ¯”å¦‚ 'Ä€' æˆ– 'Ä ' ç­‰ç­‰ã€‚
    """
    # beautiful sequence,188ä¸ªæ•´æ•°ï¼Œè¿™äº›æ•´æ•°ä»¥å…¶åŸå§‹å½¢å¼å‘ˆç°è‰¯å¥½ï¼Œä¸éœ€è¦ç§»ä½
    bs = list(range(ord("!"), ord("~")+1))+list(range(ord("Â¡"), ord("Â¬")+1))+list(range(ord("Â®"), ord("Ã¿")+1))
    
    # bsä¸­çš„æ‰€æœ‰æ•´æ•°béƒ½å°†åœ¨è¾“å‡ºå­—å…¸ä¸­ç®€å•åœ°æ˜ å°„åˆ°chr(b)
    cs = bs[:] 
    
    # ç°åœ¨è·å–éœ€è¦ç§»ä½çš„å…¶ä»–68ä¸ªæ•´æ•°çš„è¡¨ç¤ºå½¢å¼
    # æ¯ä¸ªéƒ½å°†æ˜ å°„åˆ°chr(256 + n)ï¼Œå…¶ä¸­nå°†åœ¨å¾ªç¯ä¸­ä»0...67å¢é•¿
    n = 0
    for b in range(2**8):
        if b not in bs:# å¦‚æœæ­¤å­—èŠ‚â€œéš¾çœ‹â€ï¼Œåˆ™å°†å…¶æ˜ å°„åˆ°ä¸‹ä¸€ä¸ªå¯ç”¨çš„â€œå¥½çœ‹â€å­—ç¬¦        
            bs.append(b)
            cs.append(2**8+n)
            n += 1
    cs = [chr(n) for n in cs]

    #â€œä¸ç¾è§‚â€çš„å­—ç¬¦å°†è¢«æ˜ å°„åˆ°æ‰©å±•çš„ ASCII ç  (256 ä»¥ä¸Š) çš„å­—ç¬¦ã€‚
    d = dict(zip(bs, cs))
    return d 

#è¿”å›wordä¸­åŒ…å«æ‰€æœ‰ç›¸é‚»å­—ç¬¦å¯¹çš„é›†åˆ pairs
def get_pairs(word):
    """
    è¿”å›ä¸€ä¸ªç”±å…ƒç»„ç»„æˆçš„é›†åˆï¼Œå…¶ä¸­åŒ…å«å¯è¿­ä»£å¯¹è±¡ word ä¸­æ‰€æœ‰ç›¸é‚»å…ƒç´ ç»„æˆçš„å¤§å­—æ¯ç»„åˆã€‚
    """
    pairs = set()
    prev_char = word[0]
    for char in word[1:]:
        pairs.add((prev_char, char))
        prev_char = char
    return pairs

class Encoder:

    def __init__(self, encoder, bpe_merges):
        """

        **èƒŒæ™¯**

        * **é¢„åˆ†è¯ (Pre-tokenization):** åœ¨ä½¿ç”¨åƒ BPE è¿™æ ·çš„å­è¯å•å…ƒåˆ†è¯ç®—æ³•ä¹‹å‰ï¼Œé€šå¸¸ä¼šå…ˆå¯¹æ–‡æœ¬è¿›è¡Œé¢„åˆ†è¯ã€‚é¢„åˆ†è¯ä¼šå°†æ–‡æœ¬åˆ†å‰²æˆæ›´å°çš„å•å…ƒï¼Œä¾‹å¦‚å•è¯ã€æ ‡ç‚¹ç¬¦å·ç­‰ã€‚
        * **BPE åˆå¹¶:** BPE ç®—æ³•ä¼šè¿­ä»£åœ°å°†å‡ºç°é¢‘ç‡æœ€é«˜çš„å­—ç¬¦å¯¹åˆå¹¶æˆä¸€ä¸ªæ–°çš„å­è¯å•å…ƒã€‚è¿™ä¸ªè¿‡ç¨‹ä¼šæŒç»­è¿›è¡Œï¼Œç›´åˆ°è¾¾åˆ°é¢„è®¾çš„è¯æ±‡è¡¨å¤§å°æˆ–æ»¡è¶³å…¶ä»–åœæ­¢æ¡ä»¶ã€‚
        * **é—®é¢˜:** å¦‚æœé¢„åˆ†è¯é˜¶æ®µæ²¡æœ‰è€ƒè™‘åˆ°å¤§å°å†™æ•æ„Ÿçš„æƒ…å†µï¼Œé‚£ä¹ˆ BPE ç®—æ³•å¯èƒ½ä¼šå°†åŒä¸€ä¸ªè¯çš„ä¸åŒå¤§å°å†™å½¢å¼è§†ä¸ºä¸åŒçš„å•å…ƒã€‚ä¾‹å¦‚ï¼Œ"The" å’Œ "the" å¯èƒ½ä¼šè¢«è§†ä¸ºä¸¤ä¸ªä¸åŒçš„è¯ï¼Œå³ä½¿å®ƒä»¬åœ¨è¯­ä¹‰ä¸Šæ˜¯ç›¸åŒçš„ã€‚

        **æ³¨é‡Šè§£é‡Š**

        * **`re.IGNORECASE`:**  è¿™æ˜¯ä¸€ä¸ªæ­£åˆ™è¡¨è¾¾å¼æ ‡å¿—ï¼Œè¡¨ç¤ºåœ¨è¿›è¡Œæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ—¶å¿½ç•¥å¤§å°å†™ã€‚
        * **ä½œç”¨:**  è¿™æ®µæ³¨é‡Šå»ºè®®åœ¨é¢„åˆ†è¯é˜¶æ®µä½¿ç”¨ `re.IGNORECASE` æ ‡å¿—ï¼Œä»¥ä¾¿å°†æ–‡æœ¬è½¬æ¢ä¸ºå°å†™å½¢å¼ã€‚è¿™æ ·åšå¯ä»¥ç¡®ä¿ BPE ç®—æ³•å°†åŒä¸€ä¸ªè¯çš„ä¸åŒå¤§å°å†™å½¢å¼è§†ä¸ºç›¸åŒçš„å•å…ƒï¼Œä»è€Œæé«˜åˆ†è¯çš„ä¸€è‡´æ€§å’Œæ•ˆç‡ã€‚

        **ç¤ºä¾‹**

        å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªåŒ…å«ç¼©å†™è¯ "Mr." çš„å¥å­ï¼š"Mr. Smith went to the store."

        * **ä¸ä½¿ç”¨ `re.IGNORECASE`:** é¢„åˆ†è¯å¯èƒ½ä¼šå°†å¥å­åˆ†å‰²ä¸º ["Mr.", "Smith", "went", "to", "the", "store", "."]ã€‚ç”±äº "Mr." ä¸­åŒ…å«å¤§å†™å­—æ¯ï¼ŒBPE ç®—æ³•å¯èƒ½ä¼šå°†å…¶è§†ä¸ºä¸€ä¸ªç‹¬ç«‹çš„å•å…ƒï¼Œè€Œä¸ä¼šå°†å…¶ä¸å…¶ä»–å•è¯ä¸­çš„ "mr" åˆå¹¶ã€‚
        * **ä½¿ç”¨ `re.IGNORECASE`:** é¢„åˆ†è¯ä¼šå°†å¥å­è½¬æ¢ä¸ºå°å†™å½¢å¼ï¼Œå¹¶å°†å…¶åˆ†å‰²ä¸º ["mr.", "smith", "went", "to", "the", "store", "."]ã€‚è¿™æ ·ï¼ŒBPE ç®—æ³•å°±å¯ä»¥å°† "mr." ä¸å…¶ä»–å•è¯ä¸­çš„ "mr" åˆå¹¶ï¼Œä»è€Œç”Ÿæˆæ›´ä¸€è‡´å’Œæœ‰æ•ˆçš„å­è¯å•å…ƒã€‚

        **æ€»ç»“**

        åœ¨é¢„åˆ†è¯é˜¶æ®µä½¿ç”¨ `re.IGNORECASE` æ ‡å¿—å¯ä»¥æœ‰æ•ˆåœ°è§£å†³ BPE åˆ†è¯åœ¨å¤„ç†ç¼©å†™è¯æ—¶çš„å¤§å°å†™æ•æ„Ÿé—®é¢˜ï¼Œæé«˜åˆ†è¯è´¨é‡ã€‚     

        """



        """
        è¿™æ®µæ­£åˆ™è¡¨è¾¾å¼åˆ°åº•è¦æŸ¥æ‰¾ä»€ä¹ˆï¼Ÿ
        Python æ­£åˆ™è¡¨è¾¾å¼å‚è€ƒ: https://docs.python.org/3/library/re.html
        ç«–çº¿ | è¡¨ç¤ºâ€œæˆ–â€ï¼Œå› æ­¤ re.findall ä¼šä»å·¦åˆ°å³åŒ¹é…æ–‡æœ¬ï¼Œå¹¶å°†åŒ¹é…çš„éƒ¨åˆ†åˆ†å‰²æˆå—ã€‚
        \'s ä¼šå°†ç±»ä¼¼ Andrej's çš„å†…å®¹æ‹†åˆ†ä¸º (Andrej, 's)ã€‚
        ?\p{L}+ï¼šå¯é€‰çš„ç©ºæ ¼ï¼Œåè·Ÿ 1 ä¸ªæˆ–å¤šä¸ª Unicode å­—ç¬¦ï¼Œè¿™äº›å­—ç¬¦å±äºâ€œå­—æ¯â€ç±»åˆ«ã€‚
        ?\p{N}+ï¼šå¯é€‰çš„ç©ºæ ¼ï¼Œåè·Ÿ 1 ä¸ªæˆ–å¤šä¸ª Unicode å­—ç¬¦ï¼Œè¿™äº›å­—ç¬¦å±äºâ€œæ•°å­—â€ç±»åˆ«ã€‚
        ?[^\s\p{L}\p{N}]+ï¼šå¯é€‰çš„ç©ºæ ¼ï¼Œç„¶åæ˜¯ 1 ä¸ªæˆ–å¤šä¸ªæ—¢ä¸æ˜¯ç©ºæ ¼ã€å­—æ¯ä¹Ÿä¸æ˜¯æ•°å­—çš„å­—ç¬¦ã€‚
        \s+(?!\S)ï¼š1 ä¸ªæˆ–å¤šä¸ªç©ºç™½å­—ç¬¦ï¼ˆä¾‹å¦‚ç©ºæ ¼ã€åˆ¶è¡¨ç¬¦ç­‰ï¼‰ï¼Œé™¤éå®ƒä»¬åé¢è·Ÿç€éç©ºç™½å­—ç¬¦ã€‚
        å› æ­¤ï¼Œè¿™å°†åŒ¹é…è¿ç»­çš„ç©ºç™½å­—ç¬¦åºåˆ—ï¼Œä½†æ’é™¤è¯¥åºåˆ—ä¸­çš„æœ€åä¸€ä¸ªç©ºç™½å­—ç¬¦ã€‚ æœ€åä¸€ä¸ªç©ºç™½å­—ç¬¦æœ‰æœºä¼šåŒ¹é…å‰é¢æ¨¡å¼ä¸­çš„å¯é€‰ç©ºæ ¼  ?ã€‚
        \s+ï¼š1 ä¸ªæˆ–å¤šä¸ªç©ºç™½å­—ç¬¦ï¼Œå¯èƒ½ç”¨äºæ•è·å­—ç¬¦ä¸²æœ«å°¾çš„å®Œæ•´å°¾éšç©ºç™½åºåˆ—ã€‚
        ç®€è€Œè¨€ä¹‹ï¼š   
        æˆ‘ä»¬å¯¹ä¸€äº›å¸¸è§çš„æ’‡å·ç»“æ„ï¼ˆ'sã€'tã€'re ç­‰ï¼‰è¿›è¡Œç‰¹æ®Šå¤„ç†ï¼Œå¹¶å°†å®ƒä»¬åˆ†æˆå•ç‹¬çš„æ ‡è®°ã€‚
        ç„¶åï¼Œæˆ‘ä»¬å°†å­—ç¬¦ä¸²åˆ†æˆè¿ç»­çš„å—ï¼š1) å­—æ¯ã€2) æ•°å­—ã€3) éå­—æ¯æ•°å­—ã€4) ç©ºç™½ã€‚
        æ€»çš„æ¥è¯´ï¼Œè¿™ä¸ªæ­£åˆ™è¡¨è¾¾å¼çš„ä½œç”¨æ˜¯å°†è‹±æ–‡æ–‡æœ¬åˆ†å‰²æˆå•è¯ã€æ•°å­—ã€æ ‡ç‚¹ç¬¦å·ç­‰å•å…ƒï¼Œå¹¶è¯†åˆ«å‡ºä¸€äº›å¸¸è§çš„è‹±æ–‡ç¼©å†™ã€‚
        """
        # å­—èŠ‚ç¼–ç å™¨/è§£ç å™¨   d[32] -> 'Ä '
        self.byte_encoder = bytes_to_unicode()
        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}
        
        # BPEï¼ˆå­—èŠ‚å¯¹ç¼–ç ï¼‰æ ‡è®°ç¼–ç å™¨/è§£ç å™¨  ('Ä ', 't')->[32]
        self.encoder = encoder
        self.decoder = {v:k for k,v in self.encoder.items()}
       
        # BPE åˆå¹¶åˆ—è¡¨ï¼Œå®šä¹‰ BPEâ€œæ ‘â€ï¼Œç”±è¦åˆå¹¶æˆæ ‡è®° ab çš„å…ƒç»„ (a,b) ç»„æˆ
        #>>> bpe_merges = [('a', 'b'), ('c', 'd'), ('e', 'f')]
        # >>> self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))
        # >>> self.bpe_ranks
        # {('a', 'b'): 0, ('c', 'd'): 1, ('e', 'f'): 2}
        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))
        
        # ç”¨äºé¢„åˆ†è¯çš„åˆ†å‰²æ¨¡å¼
        # åº”è¯¥æ·»åŠ  re.IGNORECASEï¼Œä»¥ä¾¿ BPE åˆå¹¶å¯ä»¥å‘ç”Ÿåœ¨ç¼©å†™çš„é¦–å­—æ¯å¤§å†™ç‰ˆæœ¬ä¸­ <-- åŸå§‹ openai æ³¨é‡Š

        self.pat = re.compile(r"""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+""")
        self.cache = {}




    def bpe(self, token):
        """
        æ­¤å‡½æ•°ä½¿ç”¨ self.bpe_ranks å°†æ‰€æœ‰å¯èƒ½çš„ BPEï¼ˆå­—èŠ‚å¯¹ç¼–ç ï¼‰æ ‡è®°è¿­ä»£åœ°åˆå¹¶åˆ°æ ‘ä¸­ã€‚
        token æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œè¡¨ç¤ºå•ä¸ªâ€œå•è¯â€ï¼ˆç»è¿‡æ­£åˆ™è¡¨è¾¾å¼åˆ†è¯åï¼‰ä»¥åŠå­—èŠ‚ç¼–ç åçš„ç»“æœï¼Œä¾‹å¦‚ â€œÄ thereâ€ã€‚
        #å‡è®¾æˆ‘ä»¬è¦å¤„ç†å¦ä¸€ä¸ªå•è¯ token = "Ä gather"ã€‚
        # å¤„ç†æµç¨‹ï¼š
        # è½¬æ¢ä¸ºå­—ç¬¦å…ƒç»„ï¼šword = ('Ä ', 'g', 'a', 't', 'h', 'e', 'r')ã€‚
        # è·å–æ‰€æœ‰ç›¸é‚»å­—ç¬¦å¯¹ï¼špairs = (('Ä ', 'g'), ('g', 'a'), ('a', 't'), ('t', 'h'), ('h', 'e'), ('e', 'r'))ã€‚
        # è¿›å…¥è¿­ä»£åˆå¹¶å¾ªç¯ï¼š
        # æŸ¥æ‰¾ pairs ä¸­æ’åæœ€é«˜çš„è¯å¯¹ï¼š('e', 'r')ï¼Œåˆå¹¶å¾—åˆ° ('Ä ', 'g', 'a', 't', 'h', 'er')ã€‚
        # å†æ¬¡æŸ¥æ‰¾æ’åæœ€é«˜çš„è¯å¯¹ï¼š('t', 'h')ï¼Œåˆå¹¶å¾—åˆ° ('Ä ', 'g', 'a', 'th', 'er')ã€‚
        # ç»§ç»­æŸ¥æ‰¾æ’åæœ€é«˜çš„è¯å¯¹ï¼šæ²¡æœ‰å…¶ä»–è¯å¯¹å‡ºç°åœ¨ self.bpe_ranks ä¸­ï¼Œå¾ªç¯ç»“æŸã€‚
        # æœ€ç»ˆç»“æœä¸º "Ä ga th er"ã€‚
        """
        # token æ˜¯å•ä¸ªâ€œå•è¯â€çš„å­—ç¬¦ä¸²ï¼Œç»è¿‡å­—èŠ‚ç¼–ç åï¼Œä¾‹å¦‚â€œÄ thereâ€ã€‚

        # è®°å¿†åŒ–ï¼Œæé«˜æ•ˆç‡
        if token in self.cache:
            return self.cache[token]

        word = tuple(token)  # å°†æ ‡è®°æ‹†åˆ†ä¸ºå•ä¸ªå­—ç¬¦çš„å…ƒç»„ï¼Œä¾‹å¦‚ ('Ä ', 't', 'h', 'e', 'r', 'e')
        pairs = get_pairs(word)  # è·å–æ‰€æœ‰ç›¸é‚»å­—ç¬¦å¯¹ï¼Œä¾‹å¦‚ (('Ä ', 't'), ('t', 'h'), ('h', 'e'), ('e', 'r'), ('r', 'e'))

        if not pairs:
            return token  # å¦‚æœæ²¡æœ‰å­—ç¬¦å¯¹ï¼Œç›´æ¥è¿”å›åŸå§‹æ ‡è®°

        while True:
            # æ‰¾åˆ°å¯ä»¥åˆå¹¶çš„ä¸‹ä¸€ä¸ªé å‰ç­‰çº§çš„å­—ç¬¦å¯¹
            # ä½¿ç”¨ lambda è¡¨è¾¾å¼æ‰¾åˆ°æ’åæœ€é å‰çš„å­—ç¬¦å¯¹34
            bigram = min(pairs, key= lambda pair: self.bpe_ranks.get(pair, float('inf')))  
            if bigram not in self.bpe_ranks:
                break  # å¦‚æœæ²¡æœ‰æ›´å¤šå­—ç¬¦å¯¹å¯ä»¥åˆå¹¶ï¼Œåˆ™é€€å‡ºå¾ªç¯

            first, second = bigram  # è·å–å­—ç¬¦å¯¹çš„ä¸¤ä¸ªå­—ç¬¦

            # æˆ‘ä»¬ç°åœ¨å°†åœ¨å½“å‰å•è¯åˆ—è¡¨ä¸­å°†æ‰€æœ‰å‡ºç°çš„ (first, second) 
            #æ›¿æ¢ä¸ºä¸€ä¸ªåˆå¹¶æ ‡è®° first_secondï¼Œå¹¶åœ¨è¾“å‡ºåˆ—è¡¨ new_word ä¸­
            new_word = []
            i = 0
            while i < len(word):
                # åœ¨å½“å‰å•è¯åºåˆ—ä¸­æŸ¥æ‰¾ä¸‹ä¸€ä¸ªå‡ºç°çš„ first
                try:
                    j = word.index(first, i)  # ä»ç´¢å¼• i å¼€å§‹æŸ¥æ‰¾å­—ç¬¦ first çš„ä½ç½®
                    new_word.extend(word[i:j])  # å°†ä» i åˆ° j çš„å­—ç¬¦æ·»åŠ åˆ° new_word
                    i = j  # å°† i æ›´æ–°ä¸º j
                except:
                    new_word.extend(word[i:])  # å¦‚æœæ‰¾ä¸åˆ° firstï¼Œåˆ™å°†å‰©ä½™å­—ç¬¦æ·»åŠ åˆ° new_word
                    break  # é€€å‡ºå¾ªç¯

                # å¦‚æœæ­¤ first åé¢è·Ÿç€ secondï¼Œåˆ™å°†å®ƒä»¬åˆå¹¶ä¸ºä¸€ä¸ª
                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:
                    new_word.append(first + second)  # åˆå¹¶ first å’Œ second å¹¶æ·»åŠ åˆ° new_word
                    i += 2  # å°† i å‘åç§»åŠ¨ä¸¤ä¸ªä½ç½®
                else:
                    new_word.append(word[i])  # å°† first æ·»åŠ åˆ° new_word
                    i += 1  # å°† i å‘åç§»åŠ¨ä¸€ä¸ªä½ç½®

            # æ‰€æœ‰å‡ºç°çš„ (first, second) éƒ½å·²åˆå¹¶ä¸º first_second
            new_word = tuple(new_word)  # å°† new_word è½¬æ¢ä¸ºå…ƒç»„
            word = new_word  # å°† word æ›´æ–°ä¸º new_word
            if len(word) == 1:
                break  # å¦‚æœ word ä¸­åªæœ‰ä¸€ä¸ªå…ƒç´ ï¼Œåˆ™é€€å‡ºå¾ªç¯
            else:
                pairs = get_pairs(word)  # æ›´æ–° pairs åˆ—è¡¨

        # å°†æ‰€æœ‰å•è¯è¿æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¹¶ä½¿ç”¨ ' ' ä½œä¸ºåˆ†éš”ç¬¦ã€‚è¯·æ³¨æ„ï¼Œ
        # åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæ‰€æœ‰å­—ç¬¦éƒ½å·²è¿›è¡Œå­—èŠ‚ç¼–ç ï¼Œä¿è¯ ' ' åœ¨å®é™…æ•°æ®ä¸­æœªä½¿ç”¨ï¼Œå¹¶ä¸”æ˜¯ä¸€ä¸ªâ€œç‰¹æ®Šâ€åˆ†éš”ç¬¦
        word = ' '.join(word)  # ä½¿ç”¨ç©ºæ ¼è¿æ¥æ‰€æœ‰å­—ç¬¦

        # ç¼“å­˜ç»“æœå¹¶è¿”å›
        self.cache[token] = word  # å°†ç»“æœç¼“å­˜åˆ° self.cache ä¸­
        return word  # è¿”å›åˆå¹¶åçš„å•è¯



    def encode(self, text):
        """ 
        è¾“å…¥å­—ç¬¦ä¸²ï¼Œè¾“å‡ºæ•´æ•°åˆ—è¡¨ï¼ˆBPE ç´¢å¼•ï¼‰

        # é¢„å¤„ç†å’Œåˆ†è¯:
        # ä½¿ç”¨ self.pat å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œå¾—åˆ° tokens = ['This', 'is', 'a', 'test', '.']ã€‚
        # å­—èŠ‚ç¼–ç å’Œè½¬æ¢:

        # éå† tokens åˆ—è¡¨ï¼Œå¯¹æ¯ä¸ª token è¿›è¡Œå¤„ç†ï¼š
        # ä¾‹å¦‚ï¼Œå¯¹äº token = 'This'ï¼š
        # è½¬æ¢ä¸ºå­—èŠ‚åºåˆ—ï¼š token_bytes = b'This'
        # ä½¿ç”¨ self.byte_encoder è½¬æ¢ä¸º Unicode å­—ç¬¦ä¸²ï¼štoken_translated = 'This'
        # å…¶ä»– token ä¹Ÿè¿›è¡Œç±»ä¼¼çš„è½¬æ¢ã€‚
        # BPE åˆå¹¶:

        # å¯¹æ¯ä¸ª token_translated åº”ç”¨ BPE åˆå¹¶è§„åˆ™ï¼š
        # ä¾‹å¦‚ï¼Œå¯¹äº token_translated = 'This'ï¼š
        # ç”±äº ('Th', 'is') çš„ä¼˜å…ˆçº§é«˜äºå…¶ä»–è¯å¯¹ï¼Œå› æ­¤å°†å®ƒä»¬åˆå¹¶ï¼Œå¾—åˆ° 'This'ã€‚
        # å¯¹äº token_translated = 'is'ï¼š
        # ç”±äº ('i', 's') å‡ºç°åœ¨ self.bpe_ranks ä¸­ï¼Œåˆå¹¶å¾—åˆ° 'is'ã€‚
        # å…¶ä»– token_translated ä¹Ÿè¿›è¡Œç±»ä¼¼çš„å¤„ç†ã€‚
        # ç´¢å¼•è½¬æ¢:

        # å°†åˆå¹¶åçš„å­è¯å•å…ƒè½¬æ¢ä¸ºè¯æ±‡è¡¨ç´¢å¼•ï¼š
        # ['This', 'is', 'a', 'test', '.'] -> [2, 3, 4, 5, 6]
        # è¾“å‡ºï¼š

        # æœ€ç»ˆè¾“å‡º BPE ç´¢å¼•åˆ—è¡¨ï¼š[2, 3, 4, 5, 6]ã€‚
        """
        bpe_idx = []  # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„ BPE ç´¢å¼•åˆ—è¡¨

        # å°†è¾“å…¥æ–‡æœ¬é¢„å…ˆåˆ†è¯ä¸ºå­—ç¬¦ä¸²æ ‡è®°ï¼ˆç²—ç•¥åœ°è¯´å°±æ˜¯å•è¯ï¼‰
        tokens = re.findall(self.pat, text)  # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ self.pat å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯
       
        # å°†æ¯ä¸ªæ ‡è®°å¤„ç†æˆ BPE æ•´æ•°
        for token in tokens:  # éå†æ¯ä¸ªæ ‡è®°
            # å°†æ ‡è®°ç¼–ç ä¸ºå­—èŠ‚ (b'') å¯¹è±¡

            """
            text = "ä½ å¥½ï¼Œä¸–ç•Œï¼"  # åŒ…å«ä¸­æ–‡çš„å­—ç¬¦ä¸²
            encoded_bytes = text.encode('utf-8') 
            print(encoded_bytes)  # è¾“å‡ºï¼šb'\xe4\xbd\xa0\xe5\xa5\xbd\xef\xbc\x8c\xe4\xb8\x96\xe7\x95\x8c\xef\xbc\x81'
            """
            token_bytes = token.encode('utf-8')  # ä½¿ç”¨ UTF-8 ç¼–ç å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºå­—èŠ‚
            # å°†æ‰€æœ‰å­—èŠ‚è½¬æ¢ä¸ºå…¶ Unicode å­—ç¬¦ä¸²è¡¨ç¤ºå½¢å¼å¹¶å±•å¹³
            token_translated = ''.join(self.byte_encoder[b] for b in token_bytes)  # ä½¿ç”¨å­—èŠ‚ç¼–ç å™¨å°†å­—èŠ‚è½¬æ¢ä¸º Unicode å­—ç¬¦
            # æ ¹æ® self.bpe_ranks æ‰§è¡Œæ‰€æœ‰é€‚ç”¨çš„ BPE åˆå¹¶
            token_merged = self.bpe(token_translated).split(' ')  # ä½¿ç”¨ BPE ç®—æ³•å¯¹è½¬æ¢åçš„æ ‡è®°è¿›è¡Œåˆå¹¶
            # å°†æ‰€æœ‰ BPE æ ‡è®°è½¬æ¢ä¸ºæ•´æ•°
            token_ix = [self.encoder[bpe_token] for bpe_token in token_merged]  # ä½¿ç”¨ç¼–ç å™¨å°† BPE æ ‡è®°è½¬æ¢ä¸ºæ•´æ•°ç´¢å¼•
            # æ‰©å±•æˆ‘ä»¬æ­£åœ¨è¿è¡Œçš„æ‰€æœ‰è¾“å‡ºæ•´æ•°åˆ—è¡¨
            bpe_idx.extend(token_ix)  # å°†è½¬æ¢åçš„æ•´æ•°ç´¢å¼•æ·»åŠ åˆ° BPE ç´¢å¼•åˆ—è¡¨ä¸­
       
        return bpe_idx  # è¿”å› BPE ç´¢å¼•åˆ—è¡¨


    def encode_and_show_work(self, text):
        """ 
        è°ƒè¯•å‡½æ•°ï¼Œä¸ encode ç›¸åŒï¼Œä½†è¿”å›æ‰€æœ‰ä¸­é—´ç»“æœ 
        ç”¨äºå°†æ–‡æœ¬ç¼–ç ä¸º BPE (Byte Pair Encoding) ç´¢å¼•åºåˆ—ï¼Œå¹¶è¿”å›æ‰€æœ‰ä¸­é—´ç»“æœï¼Œæ–¹ä¾¿è°ƒè¯•ã€‚
        """
        bpe_idx = []  # æœ€ç»ˆçš„ BPE ç´¢å¼•åˆ—è¡¨
        parts = []  # æ¯ä¸ªæ ‡è®°çš„ä¸­é—´ç»“æœåˆ—è¡¨
        tokens = re.findall(self.pat, text)  # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å¯¹æ–‡æœ¬è¿›è¡Œé¢„åˆ†è¯
        for token in tokens:  # éå†æ¯ä¸ªæ ‡è®°
            token_bytes = token.encode('utf-8')  # å°†æ ‡è®°ç¼–ç ä¸ºå­—èŠ‚
            token_translated = ''.join(self.byte_encoder[b] for b in token_bytes)  # å°†å­—èŠ‚è½¬æ¢ä¸º Unicode å­—ç¬¦
            token_merged = self.bpe(token_translated).split(' ')  # å¯¹è½¬æ¢åçš„æ ‡è®°åº”ç”¨ BPE åˆå¹¶
            token_ix = [self.encoder[bpe_token] for bpe_token in token_merged]  # å°†åˆå¹¶åçš„æ ‡è®°è½¬æ¢ä¸º BPE ç´¢å¼•
            bpe_idx.extend(token_ix)  # å°† BPE ç´¢å¼•æ·»åŠ åˆ°æœ€ç»ˆåˆ—è¡¨ä¸­
            parts.append({  # å°†ä¸­é—´ç»“æœæ·»åŠ åˆ°åˆ—è¡¨ä¸­
                'token': token,  # åŸå§‹æ ‡è®°
                'token_bytes': token_bytes,  # å­—èŠ‚è¡¨ç¤º
                'token_translated': token_translated,  # è½¬æ¢åçš„æ ‡è®°
                'token_merged': token_merged,  # åˆå¹¶åçš„æ ‡è®°
                'token_ix': token_ix,  # BPE ç´¢å¼•
            })
        out = {  # è¿”å›ç»“æœå­—å…¸
            'bpe_idx': bpe_idx,  # æœ€ç»ˆçš„ BPE ç´¢å¼•åºåˆ—
            'tokens': tokens,  # é¢„åˆ†è¯ç»“æœ
            'parts': parts,  # æ¯ä¸ªæ ‡è®°çš„ä¸­é—´ç»“æœ
        }
        return out  # è¿”å›ç»“æœå­—å…¸

    def decode(self, bpe_idx):
        """ è¾“å…¥æ•´æ•°åˆ—è¡¨ï¼Œè¾“å‡ºå­—ç¬¦ä¸² """
        # å¯¹æ•´æ•°è¿›è¡Œé€†æ˜ å°„ä»¥è·å–æ ‡è®°
        tokens_merged = [self.decoder[token] for token in bpe_idx]  
        tokens_flat = ''.join(tokens_merged)

        # åè½¬å­—èŠ‚ç¼–ç å™¨ï¼Œä¾‹å¦‚å°† 'Ä ' æ¢å¤ä¸º ' 'ï¼Œå¹¶è·å–å­—èŠ‚
        tokens_bytes = bytearray([self.byte_decoder[c] for c in tokens_flat])

        # æ¢å¤å®Œæ•´çš„ utf-8 å­—ç¬¦ä¸²
        text = tokens_bytes.decode('utf-8', errors='replace')
        return text

def get_file(local_file, remote_file):
    """ å¦‚æœéœ€è¦ï¼Œå°† remote_file ä¸‹è½½åˆ° local_file """
    if not os.path.isfile(local_file):
        print(f"æ­£åœ¨ä¸‹è½½ {remote_file} åˆ° {local_file}")
        response = requests.get(remote_file)
        open(local_file, "wb").write(response.content)

def get_encoder():
    """
    ä½œç”¨æ˜¯åŠ è½½é¢„è®­ç»ƒçš„ GPT BPE ç¼–ç å™¨/è§£ç å™¨ï¼Œå¹¶å°†å…¶å°è£…åœ¨ä¸€ä¸ª Encoder å¯¹è±¡ä¸­è¿”å›ã€‚
    è¯¥å‡½æ•°é¦–å…ˆæ£€æŸ¥æœ¬åœ°ç¼“å­˜ä¸­æ˜¯å¦å­˜åœ¨å¿…è¦çš„æ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä»è¿œç¨‹æœåŠ¡å™¨ä¸‹è½½ã€‚
    ç„¶åï¼Œå‡½æ•°åŠ è½½å¹¶è§£æè¿™äº›æ–‡ä»¶ï¼Œæœ€ç»ˆåˆ›å»ºå¹¶è¿”å›ä¸€ä¸ª Encoder å¯¹è±¡ã€‚
    """
    home_dir = os.path.expanduser('~')  # è·å–ç”¨æˆ·ä¸»ç›®å½•
    cache_dir = os.path.join(home_dir, '.cache', 'mingpt')  # ç¼“å­˜ç›®å½•
    os.makedirs(cache_dir, exist_ok=True)  # å¦‚æœç¼“å­˜ç›®å½•ä¸å­˜åœ¨åˆ™åˆ›å»º

    # åŠ è½½ encoder.jsonï¼Œå…¶ä¸­åŒ…å«ä»æ ‡è®°åˆ° BPE ç´¢å¼•çš„åŸå§‹æ˜ å°„
    encoder_local_file = os.path.join(cache_dir, 'encoder.json')  # æœ¬åœ° encoder.json æ–‡ä»¶è·¯å¾„
    encoder_remote_file = 'https://openaipublic.blob.core.windows.net/gpt-2/models/124M/encoder.json'  # è¿œç¨‹ encoder.json æ–‡ä»¶ URL
    get_file(encoder_local_file, encoder_remote_file)  # ä¸‹è½½æ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
    with open(encoder_local_file, 'r') as f:
        encoder = json.load(f)  # åŠ è½½ encoder.json æ–‡ä»¶
    assert len(encoder) == 50257  # æ–­è¨€ï¼šç¼–ç å™¨å¤§å°åº”ä¸º 50257ï¼ˆ256 ä¸ªå­—èŠ‚æ ‡è®°ï¼Œ50,000 ä¸ªåˆå¹¶æ ‡è®°å’Œ 1 ä¸ªç‰¹æ®Šçš„ <|endoftext|> æ ‡è®°ï¼‰

    # åŠ è½½ vocab.bpeï¼Œå…¶ä¸­åŒ…å« BPE åˆå¹¶ï¼Œå³ BPE æ ‘ç»“æ„
    # æ ¼å¼ä¸ºå…ƒç»„ (a, b)ï¼Œè¡¨ç¤º (a, b) å°†åˆå¹¶ä¸ºä¸€ä¸ªæ ‡è®° ab
    vocab_local_file = os.path.join(cache_dir, 'vocab.bpe')  # æœ¬åœ° vocab.bpe æ–‡ä»¶è·¯å¾„
    vocab_remote_file = 'https://openaipublic.blob.core.windows.net/gpt-2/models/124M/vocab.bpe'  # è¿œç¨‹ vocab.bpe æ–‡ä»¶ URL
    get_file(vocab_local_file, vocab_remote_file)  # ä¸‹è½½æ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
    with open(vocab_local_file, 'r', encoding="utf-8") as f:
        bpe_data = f.read()  # è¯»å– vocab.bpe æ–‡ä»¶
    # è½»é‡çº§åå¤„ç†ï¼šå»é™¤ç¬¬ä¸€è¡Œçš„ç‰ˆæœ¬å·ï¼Œæœ€åä¸€è¡Œæ˜¯ç©ºç™½è¡Œ
    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\n')[1:-1]]  # è§£æ BPE åˆå¹¶
    assert len(bpe_merges) == 50000  # æ–­è¨€ï¼šåˆå¹¶æ ‡è®°æ•°é‡åº”ä¸º 50,000

    # æ„é€ ç¼–ç å™¨å¯¹è±¡å¹¶è¿”å›
    enc = Encoder(encoder, bpe_merges)  # åˆ›å»º Encoder å¯¹è±¡
    return enc  # è¿”å›ç¼–ç å™¨å¯¹è±¡

# -----------------------------------------------------------------------------

class BPETokenizer:
    """ 
    BPETokenizer ç±»æä¾›äº†ä¸€ä¸ªæ–¹ä¾¿çš„æ¥å£ï¼Œç”¨äºåœ¨ PyTorch ç¯å¢ƒä¸­ä½¿ç”¨é¢„è®­ç»ƒçš„ BPE ç¼–ç å™¨/è§£ç å™¨ã€‚
    å®ƒå¯ä»¥å°†æ–‡æœ¬ç¼–ç ä¸º PyTorch å¼ é‡ï¼Œå¹¶å°†ç¼–ç åçš„å¼ é‡è§£ç å›æ–‡æœ¬ã€‚
    è¿™å¯¹äºå°†æ–‡æœ¬æ•°æ®é¢„å¤„ç†åè¾“å…¥åˆ° PyTorch æ¨¡å‹ä¸­éå¸¸æœ‰ç”¨ã€‚
    """

    def __init__(self):
        self.encoder = get_encoder()  # åˆå§‹åŒ–æ—¶è·å–ä¸€ä¸ªEncoderå®ä¾‹

    def __call__(self, text, return_tensors='pt'):
        # ç›®å‰ä»…æ”¯æŒPyTorchï¼›è¿™é‡Œæ˜¯ä¸ºäº†åŒ¹é…huggingface/transformersæ¥å£
        assert return_tensors == 'pt'  # æ–­è¨€ï¼šç¡®ä¿è¿”å›ç±»å‹æ˜¯'pt'ï¼ˆPyTorchå¼ é‡ï¼‰
        # ç›®å‰ä»…æ”¯æŒå•ä¸ªå­—ç¬¦ä¸²è¾“å…¥ï¼Œå°†æ¥å¯èƒ½æ”¯æŒå­—ç¬¦ä¸²åˆ—è¡¨
        assert isinstance(text, str)  # æ–­è¨€ï¼šç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²ç±»å‹
        # ç¼–ç å¹¶åˆ›å»ºä¸€ä¸ªå¤§å°ä¸º1çš„"æ‰¹æ¬¡ç»´åº¦"
        idx = [self.encoder.encode(text)]  # ä½¿ç”¨Encoderå®ä¾‹å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç 
        # å°è£…æˆPyTorchå¼ é‡
        out = torch.tensor(idx, dtype=torch.long)  # åˆ›å»ºä¸€ä¸ªPyTorchå¼ é‡
        return out  # è¿”å›PyTorchå¼ é‡

    def decode(self, idx):
        # ç¡®ä¿ç°åœ¨æ˜¯ä¸€ä¸ªç®€å•çš„1ç»´å¼ é‡
        assert idx.ndim == 1  # æ–­è¨€ï¼šç¡®ä¿è¾“å…¥æ˜¯ä¸€ä¸ª1ç»´å¼ é‡
        # å°†ç´¢å¼•è§£ç ä¸ºæ–‡æœ¬
        text = self.encoder.decode(idx.tolist())  # ä½¿ç”¨Encoderå®ä¾‹å°†ç´¢å¼•è§£ç ä¸ºæ–‡æœ¬
        return text  # è¿”å›è§£ç åçš„æ–‡æœ¬




if __name__ == '__main__':

    # è¿™æ˜¯ä¸€ä¸ªç¼–ç ç¤ºä¾‹
    text = "Hello!! I'm Andrej Karpathy. It's 2022. w00t :D ğŸ¤—"
    e = get_encoder()  # è·å–ä¸€ä¸ªç¼–ç å™¨å®ä¾‹
    r = e.encode_and_show_work(text)  # å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç å¹¶æ˜¾ç¤ºä¸­é—´æ­¥éª¤

    print("åŸå§‹æ–‡æœ¬æ˜¯ï¼š")
    print(text)
    print("é¦–å…ˆï¼Œæ–‡æœ¬ä¼šè¢«é¢„å…ˆåˆ†è¯ï¼Œåˆ†è§£æˆå—ï¼Œç»“æœæ˜¯ï¼š")
    print(r['tokens'])  # æ‰“å°é¢„åˆ†è¯åçš„æ ‡è®°åˆ—è¡¨
    # ['Hello', '!!', ' I', "'m", ' Andrej', ' Karpathy', '.', ' It', "'s", ' 2022', '.', ' w', '00', 't', ' :', 'D', ' ğŸ¤—']
    print("ç„¶åæˆ‘ä»¬è¿­ä»£æ¯ä¸ªå—å¹¶ä¾æ¬¡å¤„ç†å®ƒä»¬...")
    for part in r['parts']:
        print(part)  # æ‰“å°æ¯ä¸ªå—çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬åŸå§‹æ ‡è®°ã€å­—èŠ‚è¡¨ç¤ºã€è½¬æ¢åçš„æ ‡è®°ã€åˆå¹¶åçš„æ ‡è®°å’Œæœ€ç»ˆçš„BPEç´¢å¼•
    # {'token': 'Hello', 'token_bytes': b'Hello', 'token_translated': 'Hello', 'token_merged': ['Hello'], 'token_ix': [15496]}
    # {'token': '!!', 'token_bytes': b'!!', 'token_translated': '!!', 'token_merged': ['!!'], 'token_ix': [3228]}
    # {'token': ' I', 'token_bytes': b' I', 'token_translated': 'Ä I', 'token_merged': ['Ä I'], 'token_ix': [314]}
    # {'token': "'m", 'token_bytes': b"'m", 'token_translated': "'m", 'token_merged': ["'m"], 'token_ix': [1101]}
    # {'token': ' Andrej', 'token_bytes': b' Andrej', 'token_translated': 'Ä Andrej', 'token_merged': ['Ä Andre', 'j'], 'token_ix': [10948, 73]}
    # {'token': ' Karpathy', 'token_bytes': b' Karpathy', 'token_translated': 'Ä Karpathy', 'token_merged': ['Ä K', 'arp', 'athy'], 'token_ix': [509, 5117, 10036]}
    # {'token': '.', 'token_bytes': b'.', 'token_translated': '.', 'token_merged': ['.'], 'token_ix': [13]}
    # {'token': ' It', 'token_bytes': b' It', 'token_translated': 'Ä It', 'token_merged': ['Ä It'], 'token_ix': [632]}
    # {'token': "'s", 'token_bytes': b"'s", 'token_translated': "'s", 'token_merged': ["'s"], 'token_ix': [338]}
    # {'token': ' 2022', 'token_bytes': b' 2022', 'token_translated': 'Ä 2022', 'token_merged': ['Ä 2022'], 'token_ix': [33160]}
    # {'token': '.', 'token_bytes': b'.', 'token_translated': '.', 'token_merged': ['.'], 'token_ix': [13]}
    # {'token': ' w', 'token_bytes': b' w', 'token_translated': 'Ä w', 'token_merged': ['Ä w'], 'token_ix': [266]}
    # {'token': '00', 'token_bytes': b'00', 'token_translated': '00', 'token_merged': ['00'], 'token_ix': [405]}
    # {'token': 't', 'token_bytes': b't', 'token_translated': 't', 'token_merged': ['t'], 'token_ix': [83]}
    # {'token': ' :', 'token_bytes': b' :', 'token_translated': 'Ä :', 'token_merged': ['Ä :'], 'token_ix': [1058]}
    # {'token': 'D', 'token_bytes': b'D', 'token_translated': 'D', 'token_merged': ['D'], 'token_ix': [35]}
    # {'token': ' ğŸ¤—', 'token_bytes': b' \xf0\x9f\xa4\x97', 'token_translated': 'Ä Ã°ÅÂ¤Ä¹', 'token_merged': ['Ä Ã°Å', 'Â¤', 'Ä¹'], 'token_ix': [12520, 97, 245]}
    # (è¯·å‚è€ƒ Encoder.encode ä¸­çš„ä»£ç ï¼Œäº†è§£è¿™äº›ä¸­é—´ç»“æœæ˜¯ä»€ä¹ˆ)
    print("æœ€ç»ˆç»“æœæ˜¯è¿æ¥å¹¶å±•å¹³æ‰€æœ‰ token_ixï¼š")
    print(r['bpe_idx'])  # æ‰“å°æœ€ç»ˆçš„BPEç´¢å¼•åˆ—è¡¨
    # [15496, 3228, 314, 1101, 10948, 73, 509, 5117, 10036, 13, 632, 338, 33160, 13, 266, 405, 83, 1058, 35, 12520, 97, 245]
    # è¿™å°†æˆä¸ºTransformerçš„æ•´æ•°è¾“å…¥åºåˆ—
    print("å‡†å¤‡é¦ˆé€åˆ°Transformerï¼")
