"""
ç¡®ä¿æˆ‘ä»¬å¯ä»¥å°† huggingface/transformer GPT æ¨¡å‹åŠ è½½åˆ° minGPT ä¸­ã€‚
"""

import unittest
import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel
from mingpt.model import GPT
from mingpt.bpe import BPETokenizer

# -----------------------------------------------------------------------------

class TestHuggingFaceImport(unittest.TestCase):

    def test_gpt2(self):
        """
        æµ‹è¯• GPT2 æ¨¡å‹ã€‚
        """
        model_type = 'gpt2' # ä½¿ç”¨çš„ GPT æ¨¡å‹ç±»å‹
        device = 'cuda' if torch.cuda.is_available() else 'cpu' # ä½¿ç”¨çš„è®¾å¤‡
        prompt = "Hello!!!!!!!!!? ğŸ¤—, my dog is a little" # æµ‹è¯• prompt

        # åˆ›å»º minGPT å’Œ huggingface/transformers æ¨¡å‹
        model = GPT.from_pretrained(model_type) # ä»é¢„è®­ç»ƒæ¨¡å‹åˆ›å»º minGPT æ¨¡å‹
        model_hf = GPT2LMHeadModel.from_pretrained(model_type) # ä»é¢„è®­ç»ƒæ¨¡å‹åˆ›å»º huggingface/transformers æ¨¡å‹

        # å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡ä¸Š
        model.to(device)
        model_hf.to(device)

        # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
        model.eval()
        model_hf.eval()

        # å¯¹è¾“å…¥ prompt è¿›è¡Œåˆ†è¯
        # ... ä½¿ç”¨ minGPT åˆ†è¯å™¨
        tokenizer = BPETokenizer()
        x1 = tokenizer(prompt).to(device)
        # ... ä½¿ç”¨ huggingface/transformers åˆ†è¯å™¨
        tokenizer_hf = GPT2Tokenizer.from_pretrained(model_type)
        model_hf.config.pad_token_id = model_hf.config.eos_token_id # æŠ‘åˆ¶è­¦å‘Š
        encoded_input = tokenizer_hf(prompt, return_tensors='pt').to(device)
        x2 = encoded_input['input_ids']

        # ç¡®ä¿ logits å®Œå…¨åŒ¹é…
        logits1, loss = model(x1) # è·å– minGPT æ¨¡å‹çš„ logits
        logits2 = model_hf(x2).logits # è·å– huggingface/transformers æ¨¡å‹çš„ logits
        self.assertTrue(torch.allclose(logits1, logits2)) # æ–­è¨€ logits ç›¸ç­‰

        # ç°åœ¨ä»æ¯ä¸ªæ¨¡å‹ä¸­æŠ½å– argmax æ ·æœ¬
        y1 = model.generate(x1, max_new_tokens=20, do_sample=False)[0] # ä» minGPT æ¨¡å‹ä¸­ç”Ÿæˆæ–‡æœ¬
        y2 = model_hf.generate(x2, max_new_tokens=20, do_sample=False)[0] # ä» huggingface/transformers æ¨¡å‹ä¸­ç”Ÿæˆæ–‡æœ¬
        self.assertTrue(torch.equal(y1, y2)) # æ¯”è¾ƒåŸå§‹é‡‡æ ·ç´¢å¼•

        # å°†ç´¢å¼•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        out1 = tokenizer.decode(y1.cpu().squeeze()) # å°† minGPT æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬è§£ç ä¸ºå­—ç¬¦ä¸²
        out2 = tokenizer_hf.decode(y2.cpu().squeeze()) # å°† huggingface/transformers æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬è§£ç ä¸ºå­—ç¬¦ä¸²
        self.assertTrue(out1 == out2) # æ¯”è¾ƒè¾“å‡ºå­—ç¬¦ä¸²æ˜¯å¦ç›¸ç­‰

if __name__ == '__main__':
    unittest.main()